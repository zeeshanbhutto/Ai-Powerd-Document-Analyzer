{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f578c7e-e502-4ced-8dc4-35155b0efcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.anaconda', '.cache', '.conda', '.condarc', '.continuum', '.ipynb_checkpoints', '.ipython', '.jupyter', '.ollama', '13_12_2024.ipynb', '16_12_2024.ipynb', '20_12_2024.ipynb', 'anaconda3', 'AppData', 'Application Data', 'Assignment_2.ipynb', 'Assignment_4(function2).ipynb', 'Assignment_4(Functions).ipynb', 'Assignment_4.ipynb', 'Assig_3.ipynb', 'Chatbot_practice1.ipynb', 'Class1(9Jan2025).ipynb', 'Class10(6December2024).ipynb', 'class10{6-12-2024).ipynb', 'Class11(9Dec2024).ipynb', 'Class12(13Dec2024).ipynb', 'Class13(16Dec2024) (2).ipynb', 'Class15(20Dec2024).ipynb', 'Class16(27Dec2024).ipynb', 'Class18(Function_Copy_Decorators_Closure_Generators).ipynb', 'Class5(18Nov2024).ipynb', 'Class6(23Nove2024).ipynb', 'Class7(25Nov2024).ipynb', 'Class8(29Nove2024).ipynb', 'Class9(02Dec2024).ipynb', 'Cod_pract of All classes.ipynb', 'Contacts', 'Cookies', 'Documents', 'Downloads', 'employee.xlsx', 'faiss_index', 'Favorites', 'IntelGraphicsProfiles', 'Langchain2.ipynb', 'Langchain_1.ipynb', 'Links', 'Local Settings', 'Microsoft', 'Music', 'My Documents', 'NetHood', 'nltk_data', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{2ad838bc-efea-11ee-a54d-000d3a94eaa1}.TM.blf', 'NTUSER.DAT{2ad838bc-efea-11ee-a54d-000d3a94eaa1}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{2ad838bc-efea-11ee-a54d-000d3a94eaa1}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'Numpy Class3(17Jan2025).ipynb', 'Numpy-Class1(10Jan2025).ipynb', 'NumpyClass2(13Jan2025).ipynb', 'OneDrive', 'OOP-HackathoneTask.pdf', 'OOPS in python.ipynb', 'Pandas2(24Jan2025).ipynb', 'Pnadas-Class4(20Jan2025).ipynb', 'Practice_of_all_(1 to 13).ipynb', 'PrintHood', 'Recent', 'sample.txt', 'Saved Games', 'Searches', 'SendTo', 'Start Menu', 'Templates', 'Untitled.ipynb', 'Untitled1.ipynb', 'Videos', 'WPS Cloud Files', 'zeeshan.ipynb', 'Zeeshanbhutto.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())  # This will list all files in the current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4868a407-a1ad-44cb-b301-3e0f10686e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZEESHAN BHUTTO\n",
      "+923302571691 | zeeshanbhutto89@gmail.com |\n",
      "ï Linkdin | § GitHub |\n",
      "Karachi, Sindh - 07423, Pakistan\n",
      "OBJECTIVE\n",
      "Seeking an internship position in AI and Machine learning, Proficient in Python, deep Learning frameworks and\n",
      "skilled in building and deploying models using Sciket-learn. Possessing a strong foundation in data analysis and\n",
      "business intelligence, I am eager to contribute to data-driven decisions. Focused on creating innovative AI solutions to\n",
      "solve complex problems and deliver meaningful insights.\n",
      "EXPERIENCE\n",
      "• Company A[]\n",
      "April-2024 June-2024\n",
      "Data Science Intern at Code-Alpha\n",
      "City, Country\n",
      "◦Developed Stock Price prediction achieving forecast future prices using Y-finance library.\n",
      "◦Implemented [Machine Learning/LSTM], Data Visualization\n",
      "EDUCATION\n",
      "∗\n",
      "Sindh madressatul islam University(SMIU)\n",
      "2022 - Now\n",
      "BSCS\n",
      "Karachi, Pakistan\n",
      "PROJECTS\n",
      "·\n",
      "Project A: [House Price prediction]\n",
      "April - 2024\n",
      "Tools: [Python, ML, Linear Regression, Data Visualization]\n",
      "GitHub[§]\n",
      "·\n",
      "Project B: [C\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "pdf_path = \"Zeeshanbhutto.pdf\"  # Change to your actual file name\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(pdf_text[:1000])  # Print the first 1000 characters to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6ab086-94b6-4161-a9fc-9784795df8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c61e96d-1ca9-4d24-b77a-318b90e8eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b68b707-8bf7-4e7c-9ebf-a6b5471e7acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hadib\\AppData\\Local\\Temp\\ipykernel_8728\\2561461046.py:6: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_model = OllamaEmbeddings(model=\"gemma:2b\")  # Use \"mistral\" or \"llama2\" if needed\n",
      "Created a chunk of size 1903, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load Ollama embeddings\n",
    "embedding_model = OllamaEmbeddings(model=\"gemma:2b\")  # Use \"mistral\" or \"llama2\" if needed\n",
    "\n",
    "# Split PDF text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.create_documents([pdf_text])\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "vector_store.save_local(\"faiss_index\")  # Save the index\n",
    "\n",
    "print(\"FAISS index created and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcdefee5-7dc9-43a6-a96c-551a38f2064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "CERTIFICATIONS\n",
      "·\n",
      "Certification Intermediate Python From Data camp\n",
      "February-2024\n",
      "·\n",
      "Certification B: Intermediate SQL From Data camp\n",
      "April-2024\n",
      "·\n",
      "Certification C: Introduction to Data Analytics Coursera\n",
      "November 2023\n",
      "·\n",
      "Certification D: Python for Data Science,AI and development Coursera.\n",
      "August 2023\n",
      "ADDITIONAL INFORMATION\n",
      "Interests: LLMs, Generative AI, Business acumen, GCP AI platforms, Agile methodologies.\n",
      "\n",
      "Chunk 2:\n",
      "ZEESHAN BHUTTO\n",
      "+923302571691 | zeeshanbhutto89@gmail.com |\n",
      "ï Linkdin | § GitHub |\n",
      "Karachi, Sindh - 07423, Pakistan\n",
      "OBJECTIVE\n",
      "Seeking an internship position in AI and Machine learning, Proficient in Python, deep Learning frameworks and\n",
      "skilled in building and deploying models using Sciket-learn. Possessing a strong foundation in data analysis and\n",
      "business intelligence, I am eager to contribute to data-driven decisions. Focused on creating innovative AI solutions to\n",
      "solve complex problems and deliver meaningful insights.\n",
      "EXPERIENCE\n",
      "• Company A[]\n",
      "April-2024 June-2024\n",
      "Data Science Intern at Code-Alpha\n",
      "City, Country\n",
      "◦Developed Stock Price prediction achieving forecast future prices using Y-finance library.\n",
      "◦Implemented [Machine Learning/LSTM], Data Visualization\n",
      "EDUCATION\n",
      "∗\n",
      "Sindh madressatul islam University(SMIU)\n",
      "2022 - Now\n",
      "BSCS\n",
      "Karachi, Pakistan\n",
      "PROJECTS\n",
      "·\n",
      "Project A: [House Price prediction]\n",
      "April - 2024\n",
      "Tools: [Python, ML, Linear Regression, Data Visualization]\n",
      "GitHub[§]\n",
      "·\n",
      "Project B: [Car Price prediction]\n",
      "April - 2024\n",
      "Tools: [Python, ML, Linear Regression, Data Visualization]\n",
      "GitHub[§]\n",
      "· Developed model for House pricesions predict.\n",
      "·\n",
      "Project C: [Image Data Processing]\n",
      "April - 2024\n",
      "Tools: [Python, ML, cv2, Data Visualization]\n",
      "GitHub[§]\n",
      "· convert RGB images into Grayscale images.\n",
      "·\n",
      "Project D: [Business Intelligence Analyst]\n",
      "February-2024\n",
      "Tools: [Power BI, Python, Data pre-processing]\n",
      "GitHub[§]\n",
      "· Developed Power BI Dashboard, achieving Statistical Analysis of job Market in data Science Industry.\n",
      "SKILLS\n",
      "· Python\n",
      "· Data Visualization\n",
      "· Machine Learning:\n",
      "· Deep Learning and neural networks\n",
      "· SQL and MONGODB\n",
      "· Git and GitHub\n",
      "· Power BI\n",
      "VOLUNTEER EXPERIENCE\n",
      "·\n",
      "Volunteer Role A\n",
      "December-2023 - \"Code Jam 2.0\"\n",
      "Google Developer Student Club (GDSC-SMIU)\n",
      "[]\n",
      "· Logistics and Event Managment.\n",
      "·\n",
      "Volunteer Role B\n",
      "July-2024 - Google I/O Extended\n",
      "GDG Kolachi\n",
      "[]\n",
      "· Support team and Logistics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS index with safe deserialization\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Define query\n",
    "query = \"What is the main topic of the document?\"\n",
    "retrieved_chunks = vector_store.similarity_search(query, k=3)  # Get top 3 matches\n",
    "\n",
    "# Display the retrieved results\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "928b531b-ca35-4419-b9cf-04b54bac2367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response: This document is a resume for a candidate named Zeeshan Bhutto. The document contains information about the candidate's education, skills, and experience, as well as their volunteer experience.\n",
      "\n",
      "The candidate has a strong academic background, with degrees from Sindh Madressatul Islam University (SMIU) in 2022. They are proficient in Python, Data Visualization, and Machine Learning, including Deep Learning and neural networks. They have also acquired skills in SQL, MongoDB, Git, and GitHub.\n",
      "\n",
      "In their previous roles, the candidate has developed and implemented machine learning models for house price prediction and car price prediction, as well as a business intelligence dashboard. They are also proficient in data visualization tools such as Power BI.\n",
      "\n",
      "In their volunteer experience, the candidate has held positions in logistics and event management, as well as support and logistics for Google I/O Extended.\n",
      "\n",
      "Overall, the candidate seems to be a highly qualified and experienced individual who would be a valuable asset to any AI or Machine Learning team.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Load FAISS index\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Define query\n",
    "query = \"Explain me about this person CV or document?\"\n",
    "\n",
    "# Retrieve relevant chunks\n",
    "retrieved_chunks = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "# Combine retrieved chunks into a single prompt for Ollama\n",
    "retrieved_text = \"\\n\\n\".join([chunk.page_content for chunk in retrieved_chunks])\n",
    "\n",
    "# Define the prompt\n",
    "prompt = f\"Based on the following document excerpts, answer the question: {query}\\n\\n{retrieved_text}\"\n",
    "\n",
    "# Get response from Ollama\n",
    "response = ollama.chat(model=\"gemma:2b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(\"Chatbot Response:\", response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf52fe6-525b-43b2-98d0-ddb9f4986415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
