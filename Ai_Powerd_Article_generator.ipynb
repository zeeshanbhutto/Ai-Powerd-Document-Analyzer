{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace2c0d6-931e-4b24-9074-ea0ac1ce6c56",
   "metadata": {},
   "source": [
    "# Lets build an Ai powerd Article generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e00fb-52b6-4ce8-be27-5fbd99c76807",
   "metadata": {},
   "source": [
    "# Installing the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490a39e-8a2a-4b81-bd3a-e54343e4c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain ollama fastapi uvicorn faiss-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be53a00-a5b5-4139-a0fd-34ab74368c01",
   "metadata": {},
   "source": [
    "# LangChain Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b6e28-13c3-4dbc-b8dd-d98f0fb0ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize Ollama with the correct model\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "# Example usage\n",
    "topic = \"The Future of Artificial Intelligence\"\n",
    "article = llm.invoke(f\"Write an informative article on: {topic}\")\n",
    "\n",
    "print(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12dd183-a711-4cab-9642-291c7612eeec",
   "metadata": {},
   "source": [
    "#  (Enhance Prompts & Formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c0d95-aee5-4939-83fb-aa3bb73a66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_article(topic, length=\"medium\"):\n",
    "    length_map = {\n",
    "        \"short\": \"around 300 words\",\n",
    "        \"medium\": \"around 700 words\",\n",
    "        \"long\": \"more than 1200 words\"\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Generate a **well-structured, SEO-optimized article** on the topic: **{topic}**.\n",
    "\n",
    "    **Requirements:**\n",
    "    - Use **clear headings** (H1, H2, H3) for better readability.\n",
    "    - Ensure the article is **engaging, informative, and fact-based**.\n",
    "    - Provide **relevant citations** or examples to support key points.\n",
    "    - Optimize for **search engines (SEO)** by using relevant keywords naturally.\n",
    "    - Keep the article **{length_map.get(length, 'around 700 words')}** in length.\n",
    "    - Format the response in **Markdown** to improve structure and readability.\n",
    "\n",
    "    **Additional Enhancements:**\n",
    "    - Include an **attention-grabbing introduction** that hooks the reader.\n",
    "    - Provide a **conclusion** summarizing key takeaways.\n",
    "    - If relevant, add a **call-to-action (CTA)** encouraging further engagement.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "# Example usage\n",
    "article = generate_article(\"Impact of AI in Healthcare\", length=\"long\")\n",
    "print(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493d40e-4a64-4a3b-8f43-a36914b6ea07",
   "metadata": {},
   "source": [
    "# Implementing the Article Generator Backend (LangChain + Ollama + FastAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe0842-0dd0-4b23-a076-8d8e1e956999",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastapi uvicorn pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e82fac-e049-47f5-9e1d-5ed9993c5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9863e42-9626-4cbf-94dc-efeff7ae2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import ollama\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ArticleRequest(BaseModel):\n",
    "    topic: str\n",
    "    length: str = \"medium\"  # Options: \"short\", \"medium\", \"long\"\n",
    "\n",
    "length_map = {\n",
    "    \"short\": \"around 300 words\",\n",
    "    \"medium\": \"around 700 words\",\n",
    "    \"long\": \"1000+ words\"\n",
    "}\n",
    "\n",
    "def generate_article(topic: str, length: str) -> str:\n",
    "    \"\"\"Generates an SEO-optimized article using Ollama's LLM model (Gemma 2B).\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Generate a **well-structured, SEO-optimized article** on: **{topic}**.\n",
    "\n",
    "    **Requirements:**\n",
    "    - Use **clear headings** (H1, H2, H3) for readability.\n",
    "    - Ensure the article is **engaging, informative, and fact-based**.\n",
    "    - Optimize for **SEO** by using relevant keywords naturally.\n",
    "    - Keep the article **{length_map.get(length, 'around 700 words')}** in length.\n",
    "    - Format the response in **Markdown**.\n",
    "\n",
    "    **Enhancements:**\n",
    "    - Include an **attention-grabbing introduction**.\n",
    "    - Provide a **conclusion** summarizing key takeaways.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(model=\"gemma\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(request: ArticleRequest):\n",
    "    \"\"\"API Endpoint: Accepts a topic and returns a generated article.\"\"\"\n",
    "    article = generate_article(request.topic, request.length)\n",
    "    return {\"topic\": request.topic, \"article\": article}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545303b0-170c-4369-bcb1-69c4c0cd8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce503bd-2eae-4265-a297-de0d41e59ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Hello, FastAPI!\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be9cf6-5c36-4f95-a0f3-7df598921626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
